{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "i180419.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9KXU1Cs-G5jA",
        "dPNLWmKHG-ws",
        "Su4zJY6sHDeo",
        "NNU2H_Y8IRGO",
        "ZFRoz9FFJCAh",
        "dbxDAO-MJpwl",
        "yMLb_DfbMapr",
        "69bWYg1jL0KV",
        "9oyRG-AKMvGH",
        "M8e29xlQcsK7",
        "hum2eb6bW201",
        "_jyiarwGzauG",
        "V-2ptzfKSIPu",
        "QodRRS-CXXQU",
        "mSwbwIYJW6-l",
        "G_ePjBqq9lyr",
        "HSsSqaRr93eC",
        "ziLi44kG_cvp",
        "h9k4cK3Vc9zi",
        "26tlTF4edGG1",
        "zrQXGRVHAAKi",
        "iB4i868RPNUy",
        "9JKCFkyAGP5G"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsnDTmk1loyf"
      },
      "source": [
        "#Name: Zaynab Batool Reza\n",
        "#Email: i180419@nu.edu.pk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KXU1Cs-G5jA"
      },
      "source": [
        "## Importing Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f2UU9bkAlxS",
        "outputId": "1d107802-cb41-4015-afde-4704ba6ddfc7"
      },
      "source": [
        "!pip install -U scikit-learn"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/eb/a48f25c967526b66d5f1fa7a984594f0bf0a5afafa94a8c4dbc317744620/scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3MB 1.5MB/s \n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.24.2 threadpoolctl-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8abvTnlRVmm"
      },
      "source": [
        "import spacy as sp\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "import sklearn.metrics as sk\n",
        "import numpy as np"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NPfNtLiQvN6",
        "outputId": "a1c2dda5-209d-458b-f8bd-162f4b45bc3d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRb7KOv2RzuL",
        "outputId": "a39ba0e1-c495-4175-c94d-db9e3f850b08"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/NLP/Assignment5/Train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/NLP/Assignment5/Train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPNLWmKHG-ws"
      },
      "source": [
        "## Utility Functions to Implement Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su4zJY6sHDeo"
      },
      "source": [
        "### Fetch Training Data\n",
        "This function first declares a dictionary calles **\"classes\"** which will store all the texts against what class they belong to as their key.\n",
        "\n",
        "Firstly, it gets the names of classes by getting list of filenames in the Train folder which are Real and Fake in this scenario.\n",
        "\n",
        "Then in it adds that class to the keys of the **classes** dictionary and initializes the items against it as an empty list.\n",
        "\n",
        "Following that, each folder for each class is traversed and all the texts files are read, normalized by removing any punctuation, extra spaces or letters and then appended to the list against the class they belong to.\n",
        "\n",
        "At the end the dictionary called **classes** is returned which has list of all texts fetched for training as items against the class they belong to as keys.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGWZZucOSXFp"
      },
      "source": [
        "#get all real and fake documents as lists\n",
        "def getData():\n",
        "  %cd /content/gdrive/MyDrive/NLP/Assignment5/Train\n",
        "  punc='، ‘ ، ! ’ % ٪ ؟ : \\ “. ۔  ) ( \\ ” \\\" \\' '\n",
        "  punclist= [' ؔ','َ','ِ','ُ',]\n",
        "  for p in punclist:\n",
        "    if p not in punc:\n",
        "      punc+=\" \"+p\n",
        "  punc=punc.split()\n",
        "  alphabets='ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n",
        "  classes={}\n",
        "  for classNames in os.listdir(os.getcwd()):\n",
        "    print(\"Getting documents for \"+classNames)\n",
        "    #path='/content/gdrive/MyDrive/NLP/Assignment5/Train/'+classNames\n",
        "    %cd {classNames}\n",
        "    classes[classNames]=[]\n",
        "    for i, docName in enumerate(os.listdir(os.getcwd())):\n",
        "      # print(i)\n",
        "      #print(docName)\n",
        "      file=open(docName, 'r') \n",
        "      text=file.read()\n",
        "      # clean text of extra spaces, end of line char and punc, check code from poetry\n",
        "      text=text.replace('\\n', \" \")   \n",
        "      for char in text:\n",
        "        if char in punc:\n",
        "          text = text.replace(char, \" \")\n",
        "        if char in alphabets:\n",
        "          text=text.replace(char, \" \")\n",
        "      text= \" \".join(text.split())   \n",
        "      # cleaning ends here\n",
        "      classes[classNames].append(text)\n",
        "    %cd /content/gdrive/MyDrive/NLP/Assignment5/Train\n",
        "  return classes\n",
        "    #with open(os.path.join(os.getcwd(), filename), 'r') as f:\n",
        "\n",
        "# classes=getData()\n",
        "# print(len(classes['Real']))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNU2H_Y8IRGO"
      },
      "source": [
        "### Calculate Prior of Each Class\n",
        "This function calculates what the prior of each class is and stores it in a dictionary called classPrior against the respective class as the key.\n",
        "\n",
        "It takes the dictionary that contains all texts against their class as a parameter and firstly counts total number of texts\n",
        "\n",
        "Then it calculates the classPrior for each class by dividing the number of texts in that class with the total number of texts and stores it in the dictionary with the class Name as its key.\n",
        "\n",
        "The classPrior dictionary is then returned"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZpE2JmOWPnL"
      },
      "source": [
        "def getClassPrior(classes):\n",
        "  classPrior={}\n",
        "  totalTexts=0\n",
        "  for className in classes:\n",
        "    totalTexts+=len(classes[className])\n",
        "  for className in classes:\n",
        "    classPrior[className]=(len(classes[className]))/float(totalTexts)\n",
        "  return classPrior\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFRoz9FFJCAh"
      },
      "source": [
        "### Concatenate Texts of Each Class\n",
        "This function returns a dictionary in which the keys are the class names and the items stored against them are all the texts in that class concatenated together.\n",
        "\n",
        "It takes as input parameter the dictionary that has all texts stored against which class they belong to.\n",
        "\n",
        "Next for each class, it concatenates all the texts and then assigns the resulting string to the new **concatenatedClasses** dictionary against the class name as key"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mi53mlMFYKM5"
      },
      "source": [
        "def contatenateTexts(classesp):\n",
        "  concatenatedClasses={}\n",
        "  for className in classesp:\n",
        "    concatenatedClasses[className]=\"\"\n",
        "    for text in classesp[className]:\n",
        "      concatenatedClasses[className]+=\" \"+text\n",
        "  return concatenatedClasses\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbxDAO-MJpwl"
      },
      "source": [
        "### Calculating Conditional Probabilities of Words\n",
        "This function calculates the conditional probability of each word for each class and returns a nested dictionary which has the classes as keys. Against every class there is another dictionary which has the words as keys and their conditional probability for that class as the items.\n",
        "\n",
        "Firstly, after taking the concatenatedClasses as input parameter which is a dictionary having all texts concatenated together against the class they belong to, it extracts the vocabulary.\n",
        "\n",
        "Next, for every class in the concatenatedClasses dictionary, it initializes the inner dictionary that will contain the words as empty.\n",
        "\n",
        "Then for every word in the vocabulary, it counts number of times it appears in that class and after applying laplace smoothing and dividing it with the total number if words in that class, the conditional probability of that words in that class is saved in the nested dictionary named **wordProbabilitiesByClass**.\n",
        "\n",
        "This dictionary is then returned at the end.\n",
        "\n",
        "An example of how the probabilities are stored and accessed:\n",
        "\n",
        "wordProbabilitiesByClass={'Real':{'w1':1, 'w2':3, 'w3':2}, 'Fake':{'w1':2,'w2':2, 'w3': 1}}\n",
        "\n",
        "Then to get probability of w1 in Real, prob=wordProbabilitiesByClass['Real']['w1']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7JjmNd8b3jB"
      },
      "source": [
        "def getWordInClassProb(concatenatedClasses):\n",
        "  vocabulary=[]\n",
        "  wordProbabilitiesByClass={}\n",
        "  c=0\n",
        "  for className in concatenatedClasses:\n",
        "    print(\"In class \", className)\n",
        "    document=concatenatedClasses[className]   \n",
        "    udoc=document.split()      \n",
        "    for w in udoc:\n",
        "      if w not in vocabulary:\n",
        "        vocabulary.append(w)  \n",
        "  \n",
        "  #outer dictionary has classes as keys, in each value there is another dictionary where words are keys and their probability for that class is their value\n",
        "  for className in concatenatedClasses:\n",
        "    print(\"In class \", className)\n",
        "    wordProbabilitiesByClass[className]={}\n",
        "    print(\"Vocabulary size: \", len(vocabulary))\n",
        "    for i in range(len(vocabulary)):\n",
        "      w=vocabulary[i]\n",
        "      occurenceInClass=0\n",
        "      document=concatenatedClasses[className] \n",
        "      udoc=document.split()      \n",
        "      occurenceInClass=udoc.count(w)\n",
        "      wordProbabilitiesByClass[className][w]=(occurenceInClass+1)/float(len(concatenatedClasses[className].split())+len(vocabulary))\n",
        "\n",
        "  return wordProbabilitiesByClass, vocabulary\n",
        "  \n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMLb_DfbMapr"
      },
      "source": [
        "### Fetch Testing Data\n",
        "This function first declares a dictionary calles **\"testClasses\"** which will store all the test texts against what class they belong to as their key.\n",
        "\n",
        "Firstly, it gets the names of classes by getting list of filenames in the Test folder which are Real and Fake in this scenario.\n",
        "\n",
        "Then in it adds that class to the keys of the **testClasses** dictionary and initializes the items against it as an empty list.\n",
        "\n",
        "Following that, each folder for each class is traversed and all the texts files are read, normalized by removing any punctuation, extra spaces or letters and then appended to the list against the class they belong to.\n",
        "\n",
        "At the end the dictionary called **testClasses** is returned which has list of all texts fetched for testing as items against the class they belong to as keys."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NocAdByrxXI"
      },
      "source": [
        "def getTestData():\n",
        "  %cd /content/gdrive/MyDrive/NLP/Assignment5/Test\n",
        "  punc='، ‘ ، ! ’ % ٪ ؟ : \\ “. ۔  ) ( \\ ” \\\" \\' '\n",
        "  punclist= [' ؔ','َ','ِ','ُ',]\n",
        "  for p in punclist:\n",
        "    if p not in punc:\n",
        "      punc+=\" \"+p\n",
        "  punc=punc.split()\n",
        "  alphabets='ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n",
        "  testClasses={}\n",
        "  for classNames in os.listdir(os.getcwd()):\n",
        "    print(\"Getting documents for \"+classNames)\n",
        "    %cd {classNames}\n",
        "    testClasses[classNames]=[]\n",
        "    for i, docName in enumerate(os.listdir(os.getcwd())):\n",
        "      file=open(docName, 'r') \n",
        "      text=file.read()\n",
        "      # clean text of extra spaces, end of line char and punc, check code from poetry\n",
        "      text=text.replace('\\n', \" \")   \n",
        "      for char in text:\n",
        "        if char in punc:\n",
        "          text = text.replace(char, \" \")\n",
        "        if char in alphabets:\n",
        "          text=text.replace(char, \" \")\n",
        "      text= \" \".join(text.split())   \n",
        "      # cleaning ends here\n",
        "      testClasses[classNames].append(text)\n",
        "    %cd /content/gdrive/MyDrive/NLP/Assignment5/Test\n",
        "  return testClasses\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69bWYg1jL0KV"
      },
      "source": [
        "## Naive Bayes Training Function\n",
        "\n",
        "The function uses the functions defined above and after taking as input the dictionary containing all texts, gets the Class Priors, Conditional Probabilities and the Vocabulary for those texts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQ8-VEzQb9RQ"
      },
      "source": [
        "def NaiveBaiseTraining(classesp):\n",
        "  classPrior=getClassPrior(classesp)\n",
        "  concatenatedClasses=contatenateTexts(classesp)\n",
        "  wordProbabilitiesByClass, vocabulary=getWordInClassProb(concatenatedClasses)\n",
        "  return vocabulary,classPrior, wordProbabilitiesByClass\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oyRG-AKMvGH"
      },
      "source": [
        "## Naive Bayes Testing Function\n",
        "This function takes as input parameters, the dictionary containing all the texts for training against their class names, the vocabulary, the dictionary having class priors, the dictionary having conditional probabilities and the text the algorithm is to be tested on.\n",
        "\n",
        "First all words that also belong to vocabulary are fetched from the text.\n",
        "\n",
        "Next, a dictionary called scored is intialized with having the class name as keys and the score for that text belonging to that class against it.\n",
        "\n",
        "Initially the score for each class is the log of the class Prior of that class and then for every word extracted from the text, the log of its conditional probability for occuring in that class is added.\n",
        "\n",
        "In the end, the class with the highest score is returned as the predicted class for that text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Pz93wZbtMy7"
      },
      "source": [
        "def naiveBayesTest(testClasses, v, prior, prob, t):\n",
        "  tempwords=t.split()\n",
        "  words=[]\n",
        "  for word in tempwords:\n",
        "    if word in v:\n",
        "      words.append(word)\n",
        "  score={}\n",
        "  for className in testClasses:\n",
        "    score[className]=math.log(prior[className])\n",
        "    for w in words:\n",
        "      score[className]+=math.log(prob[className][w])\n",
        "  return max(score, key=score.get)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TZY87NgPTPV",
        "outputId": "e2b0d75e-8100-4729-aa7e-37960a2f7afa"
      },
      "source": [
        "classes=getData() #put in separate cell in future so data is fetched just once"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/NLP/Assignment5/Train\n",
            "Getting documents for Real\n",
            "/content/gdrive/MyDrive/NLP/Assignment5/Train/Real\n",
            "/content/gdrive/MyDrive/NLP/Assignment5/Train\n",
            "Getting documents for Fake\n",
            "/content/gdrive/MyDrive/NLP/Assignment5/Train/Fake\n",
            "/content/gdrive/MyDrive/NLP/Assignment5/Train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fh1R0iLSr5UD",
        "outputId": "60b31825-78e8-48d1-b404-9626d6244753"
      },
      "source": [
        "testClasses=getTestData()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/NLP/Assignment5/Test\n",
            "Getting documents for Real\n",
            "/content/gdrive/MyDrive/NLP/Assignment5/Test/Real\n",
            "/content/gdrive/MyDrive/NLP/Assignment5/Test\n",
            "Getting documents for Fake\n",
            "/content/gdrive/MyDrive/NLP/Assignment5/Test/Fake\n",
            "/content/gdrive/MyDrive/NLP/Assignment5/Test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8e29xlQcsK7"
      },
      "source": [
        "# Training "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hum2eb6bW201"
      },
      "source": [
        "## Normal Naive Baise Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6FgEXI_9Jen"
      },
      "source": [
        "Will take approximately 10-11 minutes to execute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SZ-E2KORDY0",
        "outputId": "1c119431-3923-48f4-fe0a-90e1308e5d7a"
      },
      "source": [
        "vocabulary, classPrior, wordProbabilitiesByClass=NaiveBaiseTraining(classes)\n",
        "print(len(wordProbabilitiesByClass['Real']))\n",
        "print(wordProbabilitiesByClass['Fake']['جس'])\n",
        "print(wordProbabilitiesByClass['Real']['جس'])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In class  Real\n",
            "In class  Fake\n",
            "In class  Real\n",
            "Vocabulary size:  13750\n",
            "In class  Fake\n",
            "Vocabulary size:  13750\n",
            "13750\n",
            "0.0034291477559253657\n",
            "0.002564304593543396\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jyiarwGzauG"
      },
      "source": [
        "## Stop Words Removed Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5Yr6ncgN_am"
      },
      "source": [
        "This function removes all stop words from all the texts in the input dictionary and returns the new dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2RnoU9SxAQG"
      },
      "source": [
        "def removeStopWords(classesp):\n",
        "  %cd /content/gdrive/MyDrive/NLP/Assignment5\n",
        "  f= open('stopwords-ur.txt', 'r')\n",
        "  stopWords=f.read()\n",
        "  stopWords=stopWords.split()\n",
        "  stopWordsRemovedClasses={}\n",
        "  for className in classesp:\n",
        "    textsList=copy.deepcopy(classesp[className])\n",
        "    newtextsList=[]\n",
        "    for texts in textsList:\n",
        "      newtext=\"\"\n",
        "      allWords=texts.split()\n",
        "      temp=[]\n",
        "      for word in allWords:\n",
        "        if word not in stopWords:\n",
        "          temp.append(word)\n",
        "      newtext=' '.join(temp)\n",
        "      newtextsList.append(newtext)\n",
        "    stopWordsRemovedClasses[className]=newtextsList\n",
        "\n",
        "  return stopWordsRemovedClasses\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWKTXhIfORdj"
      },
      "source": [
        "Removing stop words from training and test data sets and storing in new dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2DGpF8_zgEK",
        "outputId": "7c7ddfbe-59bd-473b-d7d7-5e9bf6825e12"
      },
      "source": [
        "stopWordsRemovedClasses=removeStopWords(classes)\n",
        "stopWordsRemovedTest=removeStopWords(testClasses)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/NLP/Assignment5\n",
            "/content/gdrive/MyDrive/NLP/Assignment5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzAEMFOU9DBS"
      },
      "source": [
        "Will take approximately 8-10 minutes to execute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnBTWU2UzlkQ",
        "outputId": "f2b5d90c-7b20-4cef-9b83-bbabdf333c1e"
      },
      "source": [
        "vocabularySW, classPriorSW, wordProbabilitiesByClassSW=NaiveBaiseTraining(stopWordsRemovedClasses)\n",
        "print(len(wordProbabilitiesByClassSW['Real']))\n",
        "print(wordProbabilitiesByClassSW['Fake']['جس'])\n",
        "print(wordProbabilitiesByClassSW['Real']['جس'])"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In class  Real\n",
            "In class  Fake\n",
            "In class  Real\n",
            "Vocabulary size:  13588\n",
            "In class  Fake\n",
            "Vocabulary size:  13588\n",
            "13588\n",
            "0.0042860024203307786\n",
            "0.0032414910858995136\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-2ptzfKSIPu"
      },
      "source": [
        "## PreProcessed Negations Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wUEVJ9zbFna"
      },
      "source": [
        "This function processes negations by removing words that come after negatory words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jX1JifvSGSB"
      },
      "source": [
        "def processNegations(classesp):\n",
        "  negationClasses={}\n",
        "  for className in classesp:\n",
        "    textsList=copy.deepcopy(classesp[className])\n",
        "    newtextsList=[]\n",
        "    for texts in textsList:\n",
        "      newtext=\"\"\n",
        "      allWords=texts.split()\n",
        "      temp = []\n",
        "      for i,word in enumerate(allWords):\n",
        "        if i>0:\n",
        "          if allWords[i-1]!='نہیں': # get rid of the word after nahi e.g انہوں نے نہیں بولا will become انہوں نے نہیں\n",
        "            temp.append(word)\n",
        "      newtext= ' '.join(temp)\n",
        "      newtextsList.append(newtext)\n",
        "    negationClasses[className]=newtextsList\n",
        "\n",
        "  return negationClasses"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pp_zbB-2XJUA"
      },
      "source": [
        "Deaking with negations in both training and testing sets and storing in new dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lu9HPdsEUETY"
      },
      "source": [
        "negationTrainingClasses=processNegations(classes)\n",
        "negationTestingClasses=processNegations(testClasses)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q74dUlDXDxq"
      },
      "source": [
        "Will take approximately 10-11 minutes to execute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqSN3ig1UQb7",
        "outputId": "1ed9b1a1-334c-4cd0-8681-aa17ff22087a"
      },
      "source": [
        "vocabularyNG, classPriorNG, wordProbabilitiesByClassNG=NaiveBaiseTraining(negationTrainingClasses)\n",
        "print(len(wordProbabilitiesByClassNG['Real']))\n",
        "print(wordProbabilitiesByClassNG['Fake']['جس'])\n",
        "print(wordProbabilitiesByClassNG['Real']['جس'])"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In class  Real\n",
            "In class  Fake\n",
            "In class  Real\n",
            "Vocabulary size:  13637\n",
            "In class  Fake\n",
            "Vocabulary size:  13637\n",
            "13637\n",
            "0.0034564085881587507\n",
            "0.0025839420273118343\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QodRRS-CXXQU"
      },
      "source": [
        "## PreProcessed Negations+Removal of StopWords Together Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sG5PGp5xXwj-"
      },
      "source": [
        "Removing stop words from both training and testing sets where negations have been preprocessed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ow6u0pvXdVz",
        "outputId": "ab7a769a-4352-4f99-976c-c3c89dca9573"
      },
      "source": [
        "stopWordsNGRemovedClasses=removeStopWords(negationTrainingClasses)\n",
        "stopWordsNGRemovedTest=removeStopWords(negationTestingClasses)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/NLP/Assignment5\n",
            "/content/gdrive/MyDrive/NLP/Assignment5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGGFZ3M2YpjL",
        "outputId": "b4367815-4281-4acd-9d69-43931c46c40a"
      },
      "source": [
        "vocabularySWNG, classPriorSWNG, wordProbabilitiesByClassSWNG=NaiveBaiseTraining(stopWordsNGRemovedClasses)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In class  Real\n",
            "In class  Fake\n",
            "In class  Real\n",
            "Vocabulary size:  13475\n",
            "In class  Fake\n",
            "Vocabulary size:  13475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSwbwIYJW6-l"
      },
      "source": [
        "## Boolean Naive Baise Training (Removing Dupliacate Words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IECW-GsOkSZ"
      },
      "source": [
        "This function removes all stop words from all the texts in the input dictionary and returns the new dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cX2vuZvW1Fc"
      },
      "source": [
        "def removeDuplicateWords(classes):\n",
        "  booleanClasses={}\n",
        "  for className in classes:\n",
        "    textsList=copy.deepcopy(classes[className])\n",
        "    newtextsList=[]\n",
        "    for texts in textsList:\n",
        "      newtext=\"\"\n",
        "      allWords=texts.split()\n",
        "      temp = []\n",
        "      for word in allWords:\n",
        "        if word not in temp:\n",
        "            temp.append(word)\n",
        "      newtext= ' '.join(temp)\n",
        "      newtextsList.append(newtext)\n",
        "    booleanClasses[className]=newtextsList\n",
        "\n",
        "  return booleanClasses"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUNl5vpQOnQt"
      },
      "source": [
        "Removing duplicate words from the training data set and storing in new dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_CN73MdXSYD"
      },
      "source": [
        "booleanClasses=removeDuplicateWords(classes)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW7OmCCS87q-"
      },
      "source": [
        "Will take approximately 5-6 minutes to execute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVobsq8JcMqV",
        "outputId": "6d6dafc9-eb92-4d7d-926e-a311624ff621"
      },
      "source": [
        "vocabulary, classPriorBNB, wordProbabilitiesByClassBNB=NaiveBaiseTraining(booleanClasses)\n",
        "print(len(wordProbabilitiesByClassBNB['Real']))\n",
        "print(wordProbabilitiesByClassBNB['Fake']['جس'])\n",
        "print(wordProbabilitiesByClassBNB['Real']['جس'])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In class  Real\n",
            "In class  Fake\n",
            "In class  Real\n",
            "Vocabulary size:  13750\n",
            "In class  Fake\n",
            "Vocabulary size:  13750\n",
            "13750\n",
            "0.0032582887051768664\n",
            "0.002679695218503431\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_ePjBqq9lyr"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSCocCYQ-B9P"
      },
      "source": [
        "Getting the list of correct labels for the documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkWZAcGk-BHo"
      },
      "source": [
        "correctLabels=[]\n",
        "for name in testClasses:\n",
        "  for text in testClasses[name]:\n",
        "    correctLabels.append(name)\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSsSqaRr93eC"
      },
      "source": [
        "## Testing the Normal Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyxvEnn9OzD1"
      },
      "source": [
        "Getting list of predicted labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGBe2BAKt2s8",
        "outputId": "5c55897b-35cc-4939-b1d7-f2852acddc0e"
      },
      "source": [
        "NBPredictedLabels=[]\n",
        "for name in testClasses:\n",
        "  for text in testClasses[name]:\n",
        "    predicted=naiveBayesTest(testClasses, vocabulary, classPrior, wordProbabilitiesByClass, text)\n",
        "    NBPredictedLabels.append(predicted)\n",
        "print(len(NBPredictedLabels))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "262\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziLi44kG_cvp"
      },
      "source": [
        "## Testing the Classifier with Stop Words Removed from Test and Training Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEgpt75-O2h_"
      },
      "source": [
        "Getting list of predicted labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nb0_IshT_lQW",
        "outputId": "524eb0a8-06f1-4145-9d3a-33feda4abdcd"
      },
      "source": [
        "SWPredictedLabels=[]\n",
        "for name in stopWordsRemovedTest:\n",
        "  for text in stopWordsRemovedTest[name]:\n",
        "    predicted=naiveBayesTest(stopWordsRemovedTest, vocabularySW, classPriorSW, wordProbabilitiesByClassSW, text)\n",
        "    SWPredictedLabels.append(predicted)\n",
        "print(len(SWPredictedLabels))"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "262\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9k4cK3Vc9zi"
      },
      "source": [
        "## Testing the Classifier with Negations having been Pre Processed (Stop Words Present)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSZr4iF-dgHd",
        "outputId": "237909d5-99d0-4997-ef5a-a5c85e2b7d9c"
      },
      "source": [
        "NGPredictedLabels=[]\n",
        "for name in negationTestingClasses:\n",
        "  for text in negationTestingClasses[name]:\n",
        "    predicted=naiveBayesTest(negationTestingClasses, vocabularyNG, classPriorNG, wordProbabilitiesByClassNG, text)\n",
        "    NGPredictedLabels.append(predicted)\n",
        "print(len(NGPredictedLabels))"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "262\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26tlTF4edGG1"
      },
      "source": [
        "## Testing the Classifier with Negations having been Pre Processed and Stop Words Removed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ie5mtwyAeKAO",
        "outputId": "b46c4e49-fce4-4538-bbfe-d5223fd396be"
      },
      "source": [
        "SWNGPredictedLabels=[]\n",
        "for name in stopWordsNGRemovedTest:\n",
        "  for text in stopWordsNGRemovedTest[name]:\n",
        "    predicted=naiveBayesTest(stopWordsNGRemovedTest, vocabularySWNG, classPriorSWNG, wordProbabilitiesByClassSWNG, text)\n",
        "    SWNGPredictedLabels.append(predicted)\n",
        "print(len(SWNGPredictedLabels))"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "262\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrQXGRVHAAKi"
      },
      "source": [
        "## Testing the Boolean Naive Bayes Algorithm with Duplicate Words Removed from Training Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCl1NGZxO4nt"
      },
      "source": [
        "Getting list of predicted labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fj2YOxuHAJlp",
        "outputId": "bfa2aed5-8735-4c0a-f4fa-1038c65fd332"
      },
      "source": [
        "BNBPredictedLabels=[]\n",
        "for name in testClasses:\n",
        "  for text in testClasses[name]:\n",
        "    predicted=naiveBayesTest(testClasses, vocabulary, classPriorBNB, wordProbabilitiesByClassBNB, text)\n",
        "    BNBPredictedLabels.append(predicted)\n",
        "print(len(BNBPredictedLabels))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "262\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB4i868RPNUy"
      },
      "source": [
        "# Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Onz5PEmgAgE9"
      },
      "source": [
        "## Comparing Accuracy of the Five Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94K00YIrE_d-"
      },
      "source": [
        "algorithms=['Normal Naive Bayes', 'StopWords Removed', 'Boolean Naive Bayes', 'PreProcessed Negations', 'PreProcessed Negations and StopWords Removed']"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNDze6bCBIPq",
        "outputId": "d8fb6eed-b3a9-4afe-b434-4628c3a4da7a"
      },
      "source": [
        "NBAccuracy=sk.accuracy_score(correctLabels, NBPredictedLabels)\n",
        "SWAccuracy=sk.accuracy_score(correctLabels, SWPredictedLabels)\n",
        "BNBAccuracy=sk.accuracy_score(correctLabels, BNBPredictedLabels)\n",
        "NGAccuracy=sk.accuracy_score(correctLabels, NGPredictedLabels)\n",
        "SWNGAccuracy=sk.accuracy_score(correctLabels, SWNGPredictedLabels)\n",
        "\n",
        "print(\"The accuracy for the normal Naive Bayes Algorithm is \", NBAccuracy)\n",
        "print(\"The accuracy for when StopWords are removed is \", SWAccuracy)\n",
        "print(\"The accuracy for Boolean Naive Bayes is \", BNBAccuracy)\n",
        "print(\"The accuracy for when Negations are preprocessed is \", NGAccuracy)\n",
        "print(\"The accuracy for when Negations are preprocessed and StopWords removed is \", SWNGAccuracy)\n",
        "print(\"\")\n",
        "print(\"Thus the best accuracy is of \", algorithms[np.argmax([NBAccuracy, SWAccuracy, BNBAccuracy, NGAccuracy, SWNGAccuracy])])"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy for the normal Naive Bayes Algorithm is  0.7022900763358778\n",
            "The accuracy for when StopWords are removed is  0.6946564885496184\n",
            "The accuracy for Boolean Naive Bayes is  0.7175572519083969\n",
            "The accuracy for when Negations are preprocessed is  0.6984732824427481\n",
            "The accuracy for when Negations are preprocessed and StopWords removed is  0.6870229007633588\n",
            "\n",
            "Thus the best accuracy is of  Boolean Naive Bayes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4y5k8oeCKpr"
      },
      "source": [
        "## Comparing the Precision of the Five Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpz6F7xSCJ-k",
        "outputId": "39826c94-2421-4e96-ce77-66b4e55546f3"
      },
      "source": [
        "NBPrecisionR=sk.precision_score(correctLabels, NBPredictedLabels, average='macro')\n",
        "SWPrecisionR=sk.precision_score(correctLabels, SWPredictedLabels, average='macro')\n",
        "BNBPrecisionR=sk.precision_score(correctLabels, BNBPredictedLabels, average='macro')\n",
        "NGPrecision=sk.precision_score(correctLabels, NGPredictedLabels, average='macro')\n",
        "SWNGPrecision=sk.precision_score(correctLabels, SWNGPredictedLabels, average='macro')\n",
        "\n",
        "print(\"The precision for the normal Naive Bayes Algorithm is \", NBPrecisionR)\n",
        "print(\"The precision for when StopWords are removed is \", SWPrecisionR)\n",
        "print(\"The precision for Boolean Naive Bayes with is \", BNBPrecisionR)\n",
        "print(\"The precision for when Negations are preprocessed is \", NGPrecision)\n",
        "print(\"The precision for when Negations are preprocessed and StopWords removed is \", SWNGPrecision)\n",
        "print(\"\")\n",
        "print(\"Thus the best precision is of \", algorithms[np.argmax([NBPrecisionR, SWPrecisionR, BNBPrecisionR, NGPrecision, SWNGPrecision])])\n"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The precision for the normal Naive Bayes Algorithm is  0.6956548198636806\n",
            "The precision for when StopWords are removed is  0.6878588516746411\n",
            "The precision for Boolean Naive Bayes with is  0.7129694835680751\n",
            "The precision for when Negations are preprocessed is  0.6916189327705757\n",
            "The precision for when Negations are preprocessed and StopWords removed is  0.6800239234449761\n",
            "\n",
            "Thus the best precision is of  Boolean Naive Bayes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JKCFkyAGP5G"
      },
      "source": [
        "## Computing Recall Score of the Five Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgGlyGj4GPky",
        "outputId": "3320c981-44d1-413b-f610-9668757dcbe1"
      },
      "source": [
        "NBf1r=sk.recall_score(correctLabels, NBPredictedLabels, average='macro')\n",
        "SWf1r=sk.recall_score(correctLabels, SWPredictedLabels, average='macro')\n",
        "BNBf1r=sk.recall_score(correctLabels, BNBPredictedLabels, average='macro')\n",
        "NGf1r=sk.recall_score(correctLabels, NGPredictedLabels, average='macro')\n",
        "SWNGf1r=sk.recall_score(correctLabels, SWNGPredictedLabels, average='macro')\n",
        "\n",
        "print(\"The recall score for the normal Naive Bayes Algorithm is \", NBf1r)\n",
        "print(\"The recall score for when StopWords are removed is \", SWf1r)\n",
        "print(\"The recall score for Boolean Naive Bayes is \", BNBf1r)\n",
        "print(\"The recall score for when Negations are preprocessed is \", NGf1r)\n",
        "print(\"The recall score for when Negations are preprocessed and StopWords removed is \", SWNGf1r)\n",
        "print(\"\")\n",
        "print(\"Thus the best recall score is of \", algorithms[np.argmax([NBf1r, SWf1r, BNBf1r, NGf1r, SWNGf1r])])\n"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The recall score for the normal Naive Bayes Algorithm is  0.6913690476190477\n",
            "The recall score for when StopWords are removed is  0.6869642857142857\n",
            "The recall score for Boolean Naive Bayes is  0.7160119047619048\n",
            "The recall score for when Negations are preprocessed is  0.6891666666666667\n",
            "The recall score for when Negations are preprocessed and StopWords removed is  0.6791666666666667\n",
            "\n",
            "Thus the best recall score is of  Boolean Naive Bayes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwV167oED1nN"
      },
      "source": [
        "## Comparing the F1 Scores of the Three Algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xa_4v2VtD1Pm",
        "outputId": "9d47657c-f728-4584-ced8-a96fd8435ca0"
      },
      "source": [
        "NBf1r=sk.f1_score(correctLabels, NBPredictedLabels, average='macro')\n",
        "SWf1r=sk.f1_score(correctLabels, SWPredictedLabels, average='macro')\n",
        "BNBf1r=sk.f1_score(correctLabels, BNBPredictedLabels, average='macro')\n",
        "NGf1r=sk.f1_score(correctLabels, NGPredictedLabels, average='macro')\n",
        "SWNGf1r=sk.f1_score(correctLabels, SWNGPredictedLabels, average='macro')\n",
        "\n",
        "print(\"The F1 score for the normal Naive Bayes Algorithm is \", NBf1r)\n",
        "print(\"The F1 score for when StopWords are removed is \", SWf1r)\n",
        "print(\"The recall score for Boolean Naive Bayes is \", BNBf1r)\n",
        "print(\"The F1 score for when Negations are preprocessed is \", NGf1r)\n",
        "print(\"The F1 score for when Negations are preprocessed and StopWords removed is \", SWNGf1r)\n",
        "print(\"\")\n",
        "print(\"Thus the best F1 score is of \", algorithms[np.argmax([NBf1r, SWf1r, BNBf1r, NGf1r, SWNGf1r])])\n"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The F1 score for the normal Naive Bayes Algorithm is  0.6928210678210678\n",
            "The F1 score for when StopWords are removed is  0.6873694886939921\n",
            "The recall score for Boolean Naive Bayes is  0.7138049126121871\n",
            "The F1 score for when Negations are preprocessed is  0.6901265064750355\n",
            "The F1 score for when Negations are preprocessed and StopWords removed is  0.6795537259113418\n",
            "\n",
            "Thus the best F1 score is of  Boolean Naive Bayes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7q2YwVLlSgh"
      },
      "source": [
        "According to the overall accuracy scores above as well as Precision, Recall and F1, the best performance is by the Boolean Naive Bayes although there is only a fractional improvement as compared to the normal Naive Bayes algorithm.\n",
        "\n",
        "When only negations are pre-processed using the above mentioned approach, the performance decreases fractionally from the normal Naive Bayes Classifier and thus this performs the third best.\n",
        "\n",
        "When Stop Words are removed from the training and testing sets, the performance actually decreases as compared to when they are kept both in Normal and PreProcessed Negations.\n",
        "\n",
        "The least best performance is when both negations are preprocessed and stop words removed as well.\n",
        "\n",
        "In order of performance the classifiers can be sorted from best to worst as:\n",
        "\n",
        "Boolean Naive Bayes\n",
        "\n",
        "Normal Naive Bayes\n",
        "\n",
        "Pre-Processed Negations\n",
        "\n",
        "StopWords Removed\n",
        "\n",
        "Pre-Processed Negations and Stop Words Removed"
      ]
    }
  ]
}